"""
ë°ì´í„° í¬ë¡¤ë§ ëª¨ë“ˆ
Amaranth ê·¸ë£¹ì›¨ì–´ì—ì„œ ì¢…ê²°ëœ ë§¤ì¶œ/ë§¤ì… í’ˆì˜ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""

from tarfile import data_filter
import time
import re
from datetime import datetime, timedelta
from typing import List, Tuple, Dict, Any
import pandas as pd
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import logging

logger = logging.getLogger(__name__)

def _clean_amount(text: str) -> int:
    """
    ê¸ˆì•¡ ë¬¸ìì—´ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    (ì‰¼í‘œ ì œê±° ë° ìˆ«ìë§Œ ì¶”ì¶œ)
    """
    if not text:
        return 0
    
    # ìˆ«ì(0-9)ì™€ ì‰¼í‘œ(,)ë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ì œê±°í•˜ê³  ì‰¼í‘œë¥¼ ì œê±°
    cleaned = re.sub(r'[^\d,]', '', text).replace(',', '')
    
    try:
        return int(cleaned)
    except ValueError:
        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ 0 ë°˜í™˜ (ë°ì´í„° ì˜¤ë¥˜ ë°©ì§€)
        return 0

def navigate_to_handover_document_list(driver):
    """
    1. 'ì „ìê²°ì¬' ë©”ë‰´ í´ë¦­
    2. 'ì¸ìˆ˜ì¸ê³„ë¬¸ì„œ' ì„œë¸Œ ë©”ë‰´ í´ë¦­ì„ í†µí•´ ìµœì¢… ëª©ì ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.
    """
    
    # 1. 'ì „ìê²°ì¬' ë©”ë‰´ í´ë¦­ (í˜ì´ì§€ ì´ë™)
    logger.info("1ë‹¨ê³„: ğŸ” 'ì „ìê²°ì¬' ë©”ë‰´ í´ë¦­ ì‹œë„ ì¤‘...")
    try:
        # ê°€ì¥ ì•ˆì •ì ì¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ XPath ì‚¬ìš©
        XPATH_ELECTRONIC_APPROVAL = "//span[text()='ì „ìê²°ì¬']"
        
        WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, XPATH_ELECTRONIC_APPROVAL))
        ).click()
        
        logger.info("âœ… 'ì „ìê²°ì¬' ë©”ë‰´ í´ë¦­ ì„±ê³µ. í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        
    except TimeoutException:
        logger.error("âŒ 1ë‹¨ê³„: 'ì „ìê²°ì¬' ë©”ë‰´ë¥¼ ì°¾ê±°ë‚˜ í´ë¦­í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Timeout)")
        return False
    except Exception as e:
        logger.error(f"âŒ 1ë‹¨ê³„: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False
    
    # --- í˜ì´ì§€ ì´ë™ í›„ ë¡œë”© ëŒ€ê¸° ë° 2ë‹¨ê³„ í´ë¦­ ì‹œì‘ ---

    logger.info("2ë‹¨ê³„: ğŸ” 'ì¸ìˆ˜ì¸ê³„' ë©”ë‰´ í™•ì¥ ë° 'ì¸ìˆ˜ì¸ê³„ë¬¸ì„œ' í´ë¦­ ì‹œë„ ì¤‘...")
    try:
        # 1. í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°: ìƒˆë¡œìš´ í˜ì´ì§€ì—ì„œ ê³ ìœ í•œ ìš”ì†Œê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
        XPATH_APPROVAL_CONTENT_AREA = "//div[@id='sideLnb']"
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.XPATH, XPATH_APPROVAL_CONTENT_AREA))
        )
        logger.info("âœ… ì „ìê²°ì¬ í˜ì´ì§€ ë‚´ë¶€ ìš”ì†Œ ë¡œë”© ì™„ë£Œ í™•ì¸.")
        
        # ì¶”ê°€ ì•ˆì •í™” ì‹œê°„
        time.sleep(2)
        
        # 2. 'ì¸ìˆ˜ì¸ê³„' ìƒìœ„ ë©”ë‰´ ì°¾ê¸° ë° í´ë¦­ (í•˜ìœ„ ë©”ë‰´ í¼ì¹˜ê¸°)
        XPATH_HANDOVER_PARENT_MENU = "//span[text()='ì¸ìˆ˜ì¸ê³„']"
        
        logger.info("ğŸ” 'ì¸ìˆ˜ì¸ê³„' ìƒìœ„ ë©”ë‰´ íƒìƒ‰ ì¤‘...")
        handover_parent = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, XPATH_HANDOVER_PARENT_MENU))
        )
        logger.info("âœ… 'ì¸ìˆ˜ì¸ê³„' ìƒìœ„ ë©”ë‰´ ìš”ì†Œ ë°œê²¬")
        
        # ìš”ì†Œê°€ ë³´ì´ë„ë¡ ìŠ¤í¬ë¡¤
        logger.info("ğŸ“œ ìš”ì†Œê°€ ë³´ì´ë„ë¡ ìŠ¤í¬ë¡¤ ì¤‘...")
        driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", handover_parent)
        time.sleep(1)
        
        # í´ë¦­ ê°€ëŠ¥í•  ë•Œê¹Œì§€ ëŒ€ê¸°
        logger.info("â³ ìš”ì†Œê°€ í´ë¦­ ê°€ëŠ¥í•  ë•Œê¹Œì§€ ëŒ€ê¸° ì¤‘...")
        WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, XPATH_HANDOVER_PARENT_MENU))
        )
        
        # í´ë¦­ ì‹œë„ (ì—¬ëŸ¬ ë°©ë²•)
        click_success = False
        
        # ë°©ë²• 1: ì¼ë°˜ í´ë¦­
        try:
            logger.info("ğŸ–±ï¸ ë°©ë²• 1: ì¼ë°˜ í´ë¦­ ì‹œë„...")
            handover_parent.click()
            logger.info("âœ… 'ì¸ìˆ˜ì¸ê³„' ìƒìœ„ ë©”ë‰´ í´ë¦­ ì„±ê³µ (ì¼ë°˜ í´ë¦­)")
            click_success = True
        except Exception as e:
            logger.warning(f"âš ï¸ ì¼ë°˜ í´ë¦­ ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 2: JavaScript í´ë¦­
        if not click_success:
            try:
                logger.info("ğŸ–±ï¸ ë°©ë²• 2: JavaScript í´ë¦­ ì‹œë„...")
                driver.execute_script("arguments[0].click();", handover_parent)
                logger.info("âœ… 'ì¸ìˆ˜ì¸ê³„' ìƒìœ„ ë©”ë‰´ í´ë¦­ ì„±ê³µ (JavaScript í´ë¦­)")
                click_success = True
            except Exception as e:
                logger.warning(f"âš ï¸ JavaScript í´ë¦­ ì‹¤íŒ¨: {e}")
        
        if not click_success:
            logger.error("âŒ ëª¨ë“  í´ë¦­ ë°©ë²• ì‹¤íŒ¨")
            return False
        
        # í•˜ìœ„ ë©”ë‰´ê°€ í¼ì³ì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸°
        logger.info("â³ í•˜ìœ„ ë©”ë‰´ í¼ì³ì§ ëŒ€ê¸° ì¤‘...")
        time.sleep(2)
        
        # 3. 'ì¸ìˆ˜ì¸ê³„ë¬¸ì„œ' ì„œë¸Œ ë©”ë‰´ í´ë¦­
        XPATH_HANDOVER_DOCUMENT = "//span[text()='ì¸ìˆ˜ì¸ê³„ë¬¸ì„œ']"
        
        logger.info("ğŸ” 'ì¸ìˆ˜ì¸ê³„ë¬¸ì„œ' ì„œë¸Œ ë©”ë‰´ íƒìƒ‰ ì¤‘...")
        handover_doc = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, XPATH_HANDOVER_DOCUMENT))
        )
        logger.info("âœ… 'ì¸ìˆ˜ì¸ê³„ë¬¸ì„œ' ì„œë¸Œ ë©”ë‰´ ìš”ì†Œ ë°œê²¬")
        
        # ìš”ì†Œê°€ ë³´ì´ë„ë¡ ìŠ¤í¬ë¡¤
        logger.info("ğŸ“œ ìš”ì†Œê°€ ë³´ì´ë„ë¡ ìŠ¤í¬ë¡¤ ì¤‘...")
        driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", handover_doc)
        time.sleep(1)
        
        # í´ë¦­ ì‹œë„ (ì—¬ëŸ¬ ë°©ë²•)
        click_success = False
        
        # ë°©ë²• 1: ì¼ë°˜ í´ë¦­
        try:
            logger.info("ğŸ–±ï¸ ë°©ë²• 1: ì¼ë°˜ í´ë¦­ ì‹œë„...")
            handover_doc.click()
            logger.info("âœ… 'ì¸ìˆ˜ì¸ê³„ë¬¸ì„œ' ì„œë¸Œ ë©”ë‰´ í´ë¦­ ì„±ê³µ (ì¼ë°˜ í´ë¦­)")
            click_success = True
        except Exception as e:
            logger.warning(f"âš ï¸ ì¼ë°˜ í´ë¦­ ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 2: JavaScript í´ë¦­
        if not click_success:
            try:
                logger.info("ğŸ–±ï¸ ë°©ë²• 2: JavaScript í´ë¦­ ì‹œë„...")
                driver.execute_script("arguments[0].click();", handover_doc)
                logger.info("âœ… 'ì¸ìˆ˜ì¸ê³„ë¬¸ì„œ' ì„œë¸Œ ë©”ë‰´ í´ë¦­ ì„±ê³µ (JavaScript í´ë¦­)")
                click_success = True
            except Exception as e:
                logger.warning(f"âš ï¸ JavaScript í´ë¦­ ì‹¤íŒ¨: {e}")
        
        if not click_success:
            logger.error("âŒ ëª¨ë“  í´ë¦­ ë°©ë²• ì‹¤íŒ¨")
            return False
        
        # ìµœì¢… í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        logger.info("â³ ì¸ìˆ˜ì¸ê³„ë¬¸ì„œ ëª©ë¡ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        time.sleep(3)
        
        logger.info("âœ…âœ…âœ… 'ì¸ìˆ˜ì¸ê³„ë¬¸ì„œ' ëª©ë¡ í˜ì´ì§€ ì´ë™ ì™„ë£Œ âœ…âœ…âœ…")
        return True
    
    except TimeoutException as te:
        logger.error(f"âŒ 2ë‹¨ê³„: íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ - {te}")
        # ë””ë²„ê¹…ì„ ìœ„í•œ ìŠ¤í¬ë¦°ìƒ· ì €ì¥
        try:
            driver.save_screenshot("debug_timeout_error.png")
            logger.info("ğŸ’¾ ë””ë²„ê¹…ìš© ìŠ¤í¬ë¦°ìƒ· ì €ì¥: debug_timeout_error.png")
        except:
            pass
        return False
    except Exception as e:
        logger.error(f"âŒ 2ë‹¨ê³„: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def get_last_12_months():
    """
    ìµœê·¼ 12ê°œì›”ì˜ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        Tuple[str, str]: (ì‹œì‘ì¼, ì¢…ë£Œì¼) YYYY-MM-DD í˜•ì‹
    """
    end_date = datetime.now()
    start_date = end_date - timedelta(days=365)  # ì•½ 12ê°œì›” ì „
    
    return start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")

def parse_date_range(start_date_str: str, end_date_str: str) -> Tuple[str, str]:
    """
    ì‚¬ìš©ì ì…ë ¥ ë‚ ì§œë¥¼ íŒŒì‹±í•˜ê³  ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        start_date_str (str): ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end_date_str (str): ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        
    Returns:
        Tuple[str, str]: (ì‹œì‘ì¼, ì¢…ë£Œì¼) YYYY-MM-DD í˜•ì‹
        
    Raises:
        ValueError: ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ëœ ê²½ìš°
    """
    try:
        start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
        end_date = datetime.strptime(end_date_str, "%Y-%m-%d")
        
        if start_date > end_date:
            raise ValueError("ì‹œì‘ ë‚ ì§œê°€ ì¢…ë£Œ ë‚ ì§œë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤")
        
        if end_date > datetime.now():
            raise ValueError("ì¢…ë£Œ ë‚ ì§œê°€ í˜„ì¬ ë‚ ì§œë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤")
            
        return start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")
        
    except ValueError as e:
        if "time data" in str(e):
            raise ValueError("ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”")
        raise

def extract_document_list(driver, start_date: str, end_date: str, doc_keyword: str) -> List[Dict[str, Any]]:
    """
    ëª©ë¡ í˜ì´ì§€ì—ì„œ íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œë“¤ì˜ ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    documents = []
    
    try:
        logger.info(f"ğŸ“„ '{doc_keyword}' í‚¤ì›Œë“œ ë¬¸ì„œ ëª©ë¡ ì¶”ì¶œ ì¤‘...")
        
        # 1. HTML ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸° ë° BeautifulSoup íŒŒì‹±
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # [UL ì»¨í…Œì´ë„ˆ íƒìƒ‰]
        document_list_container = soup.select_one('ul.tableBody') 
        
        if not document_list_container:
            logger.warning("âš ï¸ í’ˆì˜ì„œ ëª©ë¡ ì»¨í…Œì´ë„ˆ (ul.tableBody)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return documents

        # 2. LI í–‰ë“¤ ì¶”ì¶œ
        rows = document_list_container.find_all('li', recursive=False)  
        logger.info(f"ğŸ“Š ì´ {len(rows)}ê°œì˜ í–‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

        for idx, row in enumerate(rows, 1):
            try:
                # 1. ë¬¸ì„œ ì œëª© ì¶”ì¶œ
                title_element = row.select_one('.titDiv .title span')
                if not title_element:
                    title_element = row.select_one('.titDiv .title')
                    if not title_element: continue
                title = title_element.get_text(strip=True)
                
                # 2. ë¬¸ì„œë²ˆí˜¸/ë§í¬ ì¶”ì¶œ
                info_links_container = row.select_one('.infoDiv .h-box')
                if not info_links_container: continue

                info_links = info_links_container.find_all('div', class_=lambda x: x and 'txt' in x and 'infoLink' in x)
                if len(info_links) < 2: continue

                link_text_element = info_links[1] 
                link_href = link_text_element.get_text(strip=True) # í’ˆì˜ë²ˆí˜¸ í…ìŠ¤íŠ¸

                # 3. ê¸°ì•ˆì¼, ìƒíƒœ í™•ì¸ ë° í•„í„°ë§ (ìƒëµëœ ë¡œì§)
                date_text = row.select_one('.dateText').get_text(strip=True)
                status = row.select_one('.process .ellipsis2').get_text(strip=True)
                
                if 'ì¢…ê²°' not in status and 'ì™„ë£Œ' not in status: continue
                
                doc_date = parse_date_from_text(date_text)
                if not is_date_in_range(doc_date, start_date, end_date): continue
                
                # 4. í‚¤ì›Œë“œ í•„í„°ë§ ë° ë°ì´í„° êµ¬ì¡°í™”
                if doc_keyword not in title: continue
                doc_type = 'ë§¤ì¶œí’ˆì˜' if 'ë§¤ì¶œí’ˆì˜' in title else 'ë§¤ì…í’ˆì˜'

                document_data = {
                    'ê¸°ì•ˆì¼': doc_date.strftime('%Y-%m-%d'),
                    'ë¬¸ì„œì œëª©': title,
                    'ë§í¬': link_href, 
                    'êµ¬ë¶„': doc_type
                }
                
                documents.append(document_data)
                logger.debug(f"âœ… ë¬¸ì„œ ë§í¬ ì¶”ì¶œ: {title} - {doc_date.strftime('%Y-%m-%d')}")
                
            except Exception as e:
                logger.warning(f"âš ï¸ [{idx}/{len(rows)}] í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"âœ… '{doc_keyword}' í‚¤ì›Œë“œ ë¬¸ì„œ {len(documents)}ê±´ ì¶”ì¶œ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ë¬¸ì„œ ëª©ë¡ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
    
    return documents

def extract_detail_amount(driver, document_type: str) -> Dict[str, Any]:
    """
    íŒì—… ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¬ë¬´ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        driver: Selenium WebDriver ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        Dict[str, Any]: ì¶”ì¶œëœ ì¬ë¬´ ì •ë³´ (ê±°ë˜ì²˜ëª…, ê³µê¸‰ê°€ì•¡, ë¶€ê°€ì„¸, í•©ê³„ê¸ˆì•¡)
    """
    detail_data = {
        'ê±°ë˜ì²˜ëª…': '',
        'ê³µê¸‰ê°€ì•¡': 0,
        'ë¶€ê°€ì„¸': 0,
        'í•©ê³„ê¸ˆì•¡': 0
    }
    
    # try:
    #     logger.info("ğŸ” ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘...")
        
        # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
    page_source = driver.page_source
    soup = BeautifulSoup(page_source, 'html.parser')
        
    # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë ˆì´ë¸” ê¸°ë°˜ìœ¼ë¡œ ê°’ ì¶”ì¶œ
    full_text = soup.get_text()
        
    #     # ê±°ë˜ì²˜ëª… ì¶”ì¶œ
    #     account_patterns = [
    #         r'ê±°ë˜ì²˜ëª…[:\s]*([^\n]+)',
    #         r'ê±°ë˜ì²˜[:\s]*([^\n]+)',
    #         r'ê±°ë˜ì²˜ëª…[:\s]*([ê°€-í£a-zA-Z0-9\s]+)'
    #     ]
        
    #     for pattern in account_patterns:
    #         match = re.search(pattern, full_text)
    #         if match:
    #             detail_data['ê±°ë˜ì²˜ëª…'] = match.group(1).strip()
    #             break
        
    #     # ê³µê¸‰ê°€ì•¡ ì¶”ì¶œ
    #     supply_patterns = [
    #         r'ê³µê¸‰ê°€ì•¡[:\s]*([\d,]+)',
    #         r'ê³µê¸‰ê°€[:\s]*([\d,]+)',
    #         r'ê³µê¸‰ê°€ì•¡[:\s]*([0-9,]+)'
    #     ]
        
    #     for pattern in supply_patterns:
    #         match = re.search(pattern, full_text)
    #         if match:
    #             try:
    #                 detail_data['ê³µê¸‰ê°€ì•¡'] = int(match.group(1).replace(',', ''))
    #                 break
    #             except ValueError:
    #                 continue
        
    #     # ë¶€ê°€ì„¸ ì¶”ì¶œ
    #     vat_patterns = [
    #         r'ë¶€ê°€ì„¸[:\s]*([\d,]+)',
    #         r'VAT[:\s]*([\d,]+)',
    #         r'ë¶€ê°€ì„¸[:\s]*([0-9,]+)'
    #     ]
        
    #     for pattern in vat_patterns:
    #         match = re.search(pattern, full_text)
    #         if match:
    #             try:
    #                 detail_data['ë¶€ê°€ì„¸'] = int(match.group(1).replace(',', ''))
    #                 break
    #             except ValueError:
    #                 continue
        
    #     # í•©ê³„ê¸ˆì•¡ ì¶”ì¶œ
    #     total_patterns = [
    #         r'í•©ê³„ê¸ˆì•¡[:\s]*([\d,]+)',
    #         r'í•©ê³„[:\s]*([\d,]+)',
    #         r'ì´ì•¡[:\s]*([\d,]+)',
    #         r'í•©ê³„ê¸ˆì•¡[:\s]*([0-9,]+)'
    #     ]
        
    #     for pattern in total_patterns:
    #         match = re.search(pattern, full_text)
    #         if match:
    #             try:
    #                 detail_data['í•©ê³„ê¸ˆì•¡'] = int(match.group(1).replace(',', ''))
    #                 break
    #             except ValueError:
    #                 continue
        
    #     logger.info(f"âœ… ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: {detail_data}")
        
    # except Exception as e:
    #     logger.error(f"âŒ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # return detail_data
    detail_data = {'ê³µê¸‰ê°€ì•¡': 0, 'ë¶€ê°€ì„¸': 0, 'í•©ê³„ê¸ˆì•¡': 0, 'íšŒì‚¬ ì´ë¦„': 'N/A'}
    
    # 1. í•©ê³„ ì •ë³´ê°€ í¬í•¨ëœ í…Œì´ë¸” íƒìƒ‰ (ê³ ì •ëœ ìŠ¤íƒ€ì¼ ì†ì„± ì‚¬ìš©)
    total_sum_table = soup.find('table', 
        style=lambda s: s and 'border-top:2px solid rgb(102, 102, 102)' in s)
        
    if not total_sum_table:
        logger.warning("âš ï¸ í•©ê³„ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ fallback.")
        # í…Œì´ë¸”ì„ ëª» ì°¾ìœ¼ë©´ ê¸°ì¡´ì˜ ì •ê·œ í‘œí˜„ì‹ ë¡œì§ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤. (ì´í›„ ì½”ë“œì—ì„œ ì²˜ë¦¬)
        return detail_data 

    # 2. 'í•©ê³„' í…ìŠ¤íŠ¸ê°€ Bold ì²˜ë¦¬ëœ ìµœì¢… í•©ê³„ í–‰(Row) íƒìƒ‰
    final_sum_row = total_sum_table.find('tr', string=lambda t: t and 'í•©ê³„' in t)
        
    if not final_sum_row:
         logger.warning("âš ï¸ í•©ê³„ ê¸ˆì•¡ì´ ë‹´ê¸´ í–‰(Row)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
         return detail_data 
         
    # 3. í•´ë‹¹ í–‰ì˜ ëª¨ë“  ì…€(<td> ë˜ëŠ” <th>) ì¶”ì¶œ
    # NOTE: í•©ê³„ í–‰ì€ ë§ˆì§€ë§‰ <tr>ì˜ ëª¨ë“  ì…€ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    sum_cells = final_sum_row.find_all(['td', 'th'])

    # 4. ê¸ˆì•¡ ì¶”ì¶œ (ë’¤ì—ì„œ 3ê°œ ì…€)
    # [í•©ê³„ê¸ˆì•¡, ë¶€ê°€ì„¸, ê³µê¸‰ê°€ì•¡]ì´ ë’¤ì—ì„œ -1, -2, -3 ì¸ë±ìŠ¤ë¡œ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    if len(sum_cells) >= 3:
        try:
            # ğŸ’¡ ì¶”ì¶œ ëª©í‘œ: ê³µê¸‰ê°€ì•¡, ë¶€ê°€ì„¸, í•©ê³„
            detail_data['í•©ê³„ê¸ˆì•¡'] = _clean_amount(sum_cells[-1].get_text(strip=True))
            detail_data['ë¶€ê°€ì„¸'] = _clean_amount(sum_cells[-2].get_text(strip=True))
            detail_data['ê³µê¸‰ê°€ì•¡'] = _clean_amount(sum_cells[-3].get_text(strip=True))
        except Exception:
            logger.warning("âš ï¸ í•©ê³„ í–‰ì—ì„œ ê¸ˆì•¡ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì…€ ì¸ë±ìŠ¤/ê°’ í™•ì¸ í•„ìš”.")
            
    # 5. ê±°ë˜ì²˜ëª… ì¶”ì¶œ (ê°€ì¥ ìƒë‹¨ì˜ ë ˆì´ë¸” í…Œì´ë¸”ì—ì„œ ì¶”ì¶œ)
    account_label = soup.find('th', string=lambda t: t and 'ê±°ë˜ì²˜ëª…' in t)
    if account_label:
        account_value_cell = account_label.find_next_sibling('td')
        if account_value_cell:
             # ê±°ë˜ì²˜ëª…ì€ ìƒìœ„ì˜ ë‹¤ë¥¸ í…Œì´ë¸”ì— ìˆìœ¼ë¯€ë¡œ, ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë§Œ ì €ì¥í•©ë‹ˆë‹¤.
             detail_data['íšŒì‚¬ ì´ë¦„'] = account_value_cell.get_text(strip=True).split('ì™¸')[0].strip()

    return detail_data

def close_popup(driver):
    """íŒì—…ì„ ë‹«ìŠµë‹ˆë‹¤. (ESC í‚¤ ì‚¬ìš©ìœ¼ë¡œ ìµœì í™”)"""
    try:
        logger.info("ğŸšª íŒì—… ë‹«ê¸° ì‹œë„ ì¤‘...")
        from selenium.webdriver.common.keys import Keys
        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.ESCAPE)
        logger.info("âœ… ESC í‚¤ë¡œ íŒì—… ë‹«ê¸° ì„±ê³µ")
        time.sleep(1) 
        return True
    except Exception as e:
        logger.warning(f"âš ï¸ íŒì—… ë‹«ê¸° ì‹¤íŒ¨: {e}")
        return False

def run_full_crawling(driver, start_date: str, end_date: str) -> List[Dict[str, Any]]:
    """
    ì „ì²´ í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    all_data = []
    
    try:
        logger.info("ğŸš€ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
        
        keywords = ['ë§¤ì¶œí’ˆì˜', 'ë§¤ì…í’ˆì˜']
        
        for keyword in keywords:
            logger.info(f"ğŸ“‹ '{keyword}' ëª©ë¡ ì¶”ì¶œ ì¤‘...")
            
            # 1. ëª©ë¡ í˜ì´ì§€ì—ì„œ ë¬¸ì„œ ë§í¬ ì¶”ì¶œ
            document_list = extract_document_list(driver, start_date, end_date, keyword)
            
            if not document_list:
                logger.warning(f"âš ï¸ '{keyword}' ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                continue
            
            logger.info(f"âœ… '{keyword}' ë¬¸ì„œ {len(document_list)}ê±´ ë°œê²¬")
            
            # 2. ê° ë¬¸ì„œ ë§í¬ë¥¼ ìˆœíšŒí•˜ë©° ìƒì„¸ ì •ë³´ ì¶”ì¶œ (íŒì—… ì œì–´)
            for idx, doc in enumerate(document_list, 1):
                list_page_url = driver.current_url # í˜„ì¬ ëª©ë¡ í˜ì´ì§€ URL ì €ì¥ (ì˜¤ë¥˜ ë³µêµ¬ìš©)
                
                try:
                    logger.info(f"ğŸ“„ [{idx}/{len(document_list)}] {doc['ë¬¸ì„œì œëª©']} ì²˜ë¦¬ ì¤‘...")
                    
                    # --- a. ë¬¸ì„œ ì œëª© ìš”ì†Œ ì°¾ê¸° ë° í´ë¦­í•˜ì—¬ íŒì—… ë„ìš°ê¸° ---
                    XPATH_DOC_TITLE = f"//span[text()=\"{doc['ë¬¸ì„œì œëª©']}\"]" 
                    
                    title_span = WebDriverWait(driver, 10).until(
                        EC.element_to_be_clickable((By.XPATH, XPATH_DOC_TITLE))
                    )
                    
                    # JavaScript ê°•ì œ í´ë¦­ (íŒì—…ì„ ë„ìš°ëŠ” ì˜¬ë°”ë¥¸ ë™ì‘)
                    driver.execute_script("arguments[0].click();", title_span)
                    logger.info("âœ… ë¬¸ì„œ ì œëª© í´ë¦­ ì„±ê³µ. íŒì—… ë¡œë”© ëŒ€ê¸° ì¤‘...")
                    time.sleep(2) # íŒì—… ë¡œë”© ëŒ€ê¸°

                    # --- b. íŒì—… ë‚´ë¶€ ì •ë³´ ì¶”ì¶œ (ë¬¸ì„œ ì¢…ë¥˜ ì „ë‹¬) ---
                    doc_type = doc.get('êµ¬ë¶„', '')
                    detail_info = extract_detail_amount(driver, doc_type) 
                    
                    # --- c. íŒì—… ë‹«ê¸° ë° í†µí•© ---
                    close_popup(driver)
                    logger.info("âœ… íŒì—… ë‹«ê¸° ì™„ë£Œ.")
                        
                    # ëª©ë¡ í˜ì´ì§€ë¡œ ë³µê·€ (íŒì—…ì´ ë ˆì´ì–´ ëª¨ë‹¬ì´ë¯€ë¡œ driver.back() ë¶ˆí•„ìš”)
                    if driver.current_url != list_page_url:
                        driver.get(list_page_url) 
                    time.sleep(1)
                        
                    # ë¬¸ì„œ ì •ë³´ì™€ ìƒì„¸ ì •ë³´ í†µí•©
                    combined_data = {
                        **doc,
                        **detail_info
                    }
                    all_data.append(combined_data)
                    logger.info(f"âœ… [{idx}/{len(document_list)}] ë°ì´í„° í†µí•© ì™„ë£Œ")
                        
                except Exception as e:
                    logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ëª©ë¡ í˜ì´ì§€ë¡œ ê°•ì œ ë³µê·€
                    try: driver.get(list_page_url) 
                    except: pass
                    continue
        
        # 4. ìµœì¢… ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        import json
        import os
        
        output_dir = "output"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        output_file = os.path.join(output_dir, "temp_raw_data.json")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: {len(all_data)}ê±´ì˜ ë°ì´í„°ë¥¼ {output_file}ì— ì €ì¥")
        
        return pd.DataFrame(all_data)
        
    except Exception as e:
        logger.error(f"âŒ ì „ì²´ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame(columns=['ë‚ ì§œ', 'ë¬¸ì„œì œëª©', 'êµ¬ë¶„', 'ê³µê¸‰ê°€ì•¡'])

def parse_date_from_text(date_text: str) -> datetime:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤. (YYYY-MM-DD ë˜ëŠ” MM-DD í˜•ì‹ ì§€ì›)
    
    Args:
        date_text (str): ë‚ ì§œê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ ('10-17 (ê¸ˆ)', '2025.10.17' ë“±)
        
    Returns:
        datetime: íŒŒì‹±ëœ ë‚ ì§œ
        
    Raises:
        ValueError: íŒŒì‹±ì— ì‹¤íŒ¨í–ˆì„ ê²½ìš°
    """
    
    # ë¶ˆí•„ìš”í•œ ê³µë°±, ê´„í˜¸, ìš”ì¼ ì •ë³´ ì œê±° (ì˜ˆ: '10-17 (ê¸ˆ)' -> '10-17')
    cleaned_text = re.sub(r'\s*\(.+\)', '', date_text).strip()
    
    # 1. ì›”-ì¼ í˜•ì‹ íŒŒì‹± ë¡œì§ ì¶”ê°€ (ëª©ë¡ í˜ì´ì§€ í˜•ì‹)
    month_day_pattern = r'(\d{1,2})[.-]\s*(\d{1,2})' 
    match_md = re.search(month_day_pattern, cleaned_text)
    
    if match_md:
        month, day = match_md.groups()
        current_year = datetime.now().year
        try:
            # ì—°ë„ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ í˜„ì¬ ì—°ë„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            return datetime(current_year, int(month), int(day))
        except ValueError:
            pass # ì˜ëª»ëœ ì›”/ì¼ì´ë©´ ë‹¤ìŒ íŒ¨í„´ ì‹œë„ (ë§¤ìš° ë“œë­„)

    
    # 2. ê¸°ì¡´ ì—°ë„ í¬í•¨ íŒ¨í„´ ì‹œë„
    date_patterns = [
        r'(\d{4})[.-](\d{1,2})[.-](\d{1,2})', # YYYY-MM-DD, YYYY.MM.DD
        r'(\d{4})/(\d{1,2})/(\d{1,2})',# YYYY/MM/DD
        r'(\d{1,2})[.-](\d{1,2})[.-](\d{4})',# MM-DD-YYYY, MM.DD.YYYY
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, cleaned_text)
        if match:
            groups = match.groups()
            if len(groups) == 3:
                try:
                    if len(groups[0]) == 4: # YYYY-MM-DD í˜•ì‹
                        year, month, day = groups
                    else: # MM-DD-YYYY í˜•ì‹
                        month, day, year = groups
                    
                    return datetime(int(year), int(month), int(day))
                except ValueError:
                    continue
    
    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ, ê¸°ë³¸ê°’ ë°˜í™˜ ëŒ€ì‹  ì˜¤ë¥˜ ë°œìƒ (ë””ë²„ê¹… ì§€ì›)
    raise ValueError(f"ë‚ ì§œ í…ìŠ¤íŠ¸ íŒŒì‹± ì‹¤íŒ¨: í˜•ì‹ '{date_text}'")

def is_date_in_range(date: datetime, start_date: str, end_date: str) -> bool:
    """
    ë‚ ì§œê°€ ì§€ì •ëœ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    
    Args:
        date (datetime): í™•ì¸í•  ë‚ ì§œ
        start_date (str): ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end_date (str): ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        
    Returns:
        bool: ë²”ìœ„ ë‚´ ì—¬ë¶€
    """
    try:
        start = datetime.strptime(start_date, "%Y-%m-%d")
        end = datetime.strptime(end_date, "%Y-%m-%d")
        return start <= date <= end
    except ValueError:
        return False

def crawl_all_data(driver, start_date: str, end_date: str) -> pd.DataFrame:
    """
    ëª¨ë“  ë§¤ì¶œ/ë§¤ì… ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        driver: Selenium WebDriver ì¸ìŠ¤í„´ìŠ¤
        start_date (str): ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end_date (str): ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        
    Returns:
        pd.DataFrame: ì¶”ì¶œëœ ë°ì´í„° (ì»¬ëŸ¼: ['ë‚ ì§œ', 'ë¬¸ì„œì œëª©', 'êµ¬ë¶„', 'ê³µê¸‰ê°€ì•¡'])
    """
    try:
        logger.info("ğŸš€ ì „ì²´ ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘")
        
        # í’ˆì˜ì„œ ëª©ë¡ í˜ì´ì§€ë¡œ ì´ë™ (ì´ë¯¸ navigate_to_handover_document_listë¡œ ì´ë™í–ˆìœ¼ë¯€ë¡œ ìŠ¤í‚µ)
        # if not navigate_to_handover_document_list(driver):
        #     logger.error("âŒ í’ˆì˜ì„œ ëª©ë¡ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨")
        #     return pd.DataFrame(columns=['ë‚ ì§œ', 'ë¬¸ì„œì œëª©', 'êµ¬ë¶„', 'ê³µê¸‰ê°€ì•¡'])
        
        all_documents = []
        
        # ë§¤ì¶œ ë¬¸ì„œ ì¶”ì¶œ
        logger.info("ğŸ’° ë§¤ì¶œ ë¬¸ì„œ ì¶”ì¶œ ì¤‘...")
        sales_docs = extract_completed_documents(driver, start_date, end_date, 'ë§¤ì¶œ')
        all_documents.extend(sales_docs)
        
        # ë§¤ì… ë¬¸ì„œ ì¶”ì¶œ
        logger.info("ğŸ’¸ ë§¤ì… ë¬¸ì„œ ì¶”ì¶œ ì¤‘...")
        purchase_docs = extract_completed_documents(driver, start_date, end_date, 'ë§¤ì…')
        all_documents.extend(purchase_docs)
        
        # DataFrame ìƒì„±
        if all_documents:
            df = pd.DataFrame(all_documents)
            df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
            df = df.sort_values('ë‚ ì§œ')
            logger.info(f"âœ… ì´ {len(df)}ê±´ì˜ ë°ì´í„° í¬ë¡¤ë§ ì™„ë£Œ")
        else:
            df = pd.DataFrame(columns=['ë‚ ì§œ', 'ë¬¸ì„œì œëª©', 'êµ¬ë¶„', 'ê³µê¸‰ê°€ì•¡'])
            logger.warning("âš ï¸ ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        return df
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame(columns=['ë‚ ì§œ', 'ë¬¸ì„œì œëª©', 'êµ¬ë¶„', 'ê³µê¸‰ê°€ì•¡'])

